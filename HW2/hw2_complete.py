# -*- coding: utf-8 -*-
"""hw2_complete-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJBfa_ursj0YgCMtvIASj9DHHv17XJmU
"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, DepthwiseConv2D
from keras.losses import SparseCategoricalCrossentropy
import numpy as np

#create model
model1 = Sequential()
#add model layers
model1.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32,32,3)))
model1.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))
model1.add(MaxPooling2D((2, 2), strides=(2, 2)))
model1.add(Flatten())
model1.add(Dense(1024, activation='softmax'))
model1.add(Dense(10, activation='softmax'))

#compile the model and choose loss function and optimizer
model1.compile(optimizer='adam',
              loss=SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

#for calculating output shape I have defined some functions
import numpy as np

kernel_size = 3
stride = 2 
input_shape = 32
padding = 0 

def conv_height_width_cal(input_shape, padding, kernel_size):
  return np.ceil((input_shape+padding-kernel_size)/(stride)) + 1

def flatten_height_width_cal(input_h, input_w, channel):
  return input_h*input_w*channel
#first conv layer output height and width
Conv2D_1_output = conv_height_width_cal (32, 0, 3)
Conv2D_1_output

#second conv layer output height and width
Conv2D_2_output = conv_height_width_cal (16, 0, 3)
Conv2D_2_output

#flatten layer output shape 
flatten_output = flatten_height_width_cal (4, 4, 64)
flatten_output

#other layers do not need calculations

#calculating number of parameters for each layer 
def conv_param (kernel_size, number_of_filters, channel):
  return kernel_size*kernel_size*number_of_filters*channel + number_of_filters

def dense_param (input_shape, output_shape):
  return input_shape*output_shape+output_shape

#first conv layer parameters
Conv2D_1_param = conv_param (3, 32, 3)
Conv2D_1_param

#second conv  layer parameters
Conv2D_2_param = conv_param (3, 64, 32)
Conv2D_2_param

#first dense layer parameters
dense_8_param = dense_param (1024, 1024)
dense_8_param

#final dense layer parameters
dense_9_param = dense_param (1024, 10)
dense_9_param

#calculating number of macs for each layer 
def conv_macs (input_shape, kernel_size, number_of_filters, channel, padding):
  return kernel_size*kernel_size*number_of_filters*channel * (np.ceil((input_shape+padding-kernel_size)/(stride)) + 1) * (np.ceil((input_shape+padding-kernel_size)/(stride)) + 1)

def dense_macs (input_shape, output_shape):
  return input_shape*output_shape

#first conv layer macs
Conv2D_1_macs = conv_macs (32, 3, 32, 3, 0)
Conv2D_1_macs

#second conv  layer macs
Conv2D_2_macs = conv_macs (16, 3, 64, 32, 0)
Conv2D_2_macs

#first dense layer macs
dense_8_macs = dense_macs (1024, 1024)
dense_8_macs

#final dense layer parameters
dense_9_macs = dense_macs (1024, 10)
dense_9_macs

model1.summary()

#importing the Cifar10 dataset from keras
from keras.datasets import cifar10
#load dataset
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

# Now separate out a validation set.
val_frac = 0.1
num_val_samples = int(len(train_images)*val_frac)
# choose num_val_samples indices up to the size of train_images, !replace => no repeats
val_idxs = np.random.choice(np.arange(len(train_images)), size=num_val_samples, replace=False)
trn_idxs = np.setdiff1d(np.arange(len(train_images)), val_idxs)
val_images = train_images[val_idxs, :,:,:]
train_images = train_images[trn_idxs, :,:,:]

val_labels = train_labels[val_idxs]
train_labels = train_labels[trn_idxs]

train_images.shape, val_images.shape, test_images.shape

train_labels.shape, val_labels.shape, test_labels.shape

#removing the extra dimension of the labels 
train_labels = train_labels.squeeze()
test_labels = test_labels.squeeze()
val_labels = val_labels.squeeze()

#mapping the input values from 0-255 to 0-1
train_images = train_images / 255.0
test_images  = test_images  / 255.0
val_images   = val_images   / 255.0
print("Training Images range from {:2.5f} to {:2.5f}".format(np.min(train_images), np.max(train_images)))
print("Test     Images range from {:2.5f} to {:2.5f}".format(np.min(test_images), np.max(test_images)))

train_labels.shape, val_labels.shape, test_labels.shape

history = model1.fit(train_images, train_labels, 
                       validation_data=(val_images, val_labels),
                       epochs=50)
model1.save('saved_models/volume_conv1')

test_loss, test_acc = model1.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

import matplotlib.pyplot as plt

plt.subplot(2,1,1)
plt.plot(history.epoch, history.history['accuracy'], 
        history.epoch, history.history['val_accuracy'])
plt.legend(['Train Accuracy', 'Val Accuracy'])
plt.grid(True)
plt.subplot(2,1,2)
plt.plot(history.epoch, history.history['loss'],
        history.epoch, history.history['val_loss'])
plt.grid(True)
plt.legend(['Train Loss', 'Val Loss'])

from PIL import Image
import PIL

#loading the image using Pillow library
img = Image.open('cat.jpg')
img

img.size

#cropping the image to appropriate size
img_cropped = img.crop((5, 120, 505, 620))
img_cropped.size

img_cropped

#reszizing theimage to input size 
img_resized = img_cropped.resize((32, 32))
img_resized.size

img_resized

img_resized.save('test_image_cat.jpg')

#converting to numpy array 

img_arr =  np.asarray(img_resized)
img_arr.shape

img_arr = np.expand_dims(img_arr, axis=0)
img_arr = img_arr/255.0
img_arr.shape

#feeding the image to the model. It will return 10 elements and each of them 
#show the probability of belonging to each class
prediction = model1.predict(img_arr)
print(prediction)
class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']
print("This is an image of", class_names[np.argmax(prediction)], ".")

#create model
model2 = Sequential()
#add model layers
model2.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2, 2), padding='same', input_shape=(32,32,3)))
model2.add(Conv2D(32, kernel_size=(1, 1), strides=(1, 1), activation='relu'))
#model1.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu', input_shape=(32,32,3)))
#model1.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'))
model2.add(DepthwiseConv2D(kernel_size=(3,3), strides=(2, 2), padding='same', input_shape=(32,32,3)))
model2.add(Conv2D(64, kernel_size=(1, 1), strides=(1, 1), activation='relu'))
model2.add(MaxPooling2D((2, 2), strides=(2, 2)))
model2.add(Flatten())
model2.add(Dense(1024, activation='softmax'))
model2.add(Dense(10, activation='softmax'))

#compile the model and choose loss function and optimizer
model2.compile(optimizer='adam',
              loss=SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model2.summary()

history2 = model2.fit(train_images, train_labels, 
                       validation_data=(val_images, val_labels),
                       epochs=50)
model1.save('saved_models/volume_conv2')



test_loss, test_acc = model1.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

import matplotlib.pyplot as plt

plt.subplot(2,1,1)
plt.plot(history2.epoch, history2.history['accuracy'], 
        history2.epoch, history2.history['val_accuracy'])
plt.legend(['Train Accuracy', 'Val Accuracy'])
plt.grid(True)
plt.subplot(2,1,2)
plt.plot(history2.epoch, history2.history['loss'],
        history2.epoch, history2.history['val_loss'])
plt.grid(True)
plt.legend(['Train Loss', 'Val Loss'])

#feeding the image to the model. It will return 10 elements and each of them 
#show the probability of belonging to each class
prediction = model2.predict(img_arr)
print(prediction)
class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']
print("This is an image of", class_names[np.argmax(prediction)], ".")

